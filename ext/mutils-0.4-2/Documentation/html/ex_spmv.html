
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Parallel sparse matrix - vector multiplication</title><meta name="generator" content="MATLAB 7.13"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2013-06-19"><meta name="DC.source" content="ex_spmv.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Parallel sparse matrix - vector multiplication</h1><!--introduction--><p>ex_spmv shows how to</p><div><ul><li>reorder the sparse matrix to be suited for parallel spmv</li><li>distribute the matrix among threads using sparse_convert</li><li>run parallel spmv on native MATLAB matrices</li></ul></div><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#3">Generate unstructured triangular mesh</a></li><li><a href="#4">Build sparse matrices</a></li><li><a href="#7">Compute communication reducing reordering and permute the matrices</a></li><li><a href="#10">Parallel sparse matrix - vector multiplication</a></li></ul></div><pre class="codeinput"><span class="keyword">function</span> ex_spmv
</pre><h2>Generate unstructured triangular mesh<a name="3"></a></h2><pre class="codeinput"><span class="comment">% triangle options</span>
opts = [];
opts.max_tri_area = 0.000084;
opts.element_type = <span class="string">'tri7'</span>;
opts.gen_edges = 0;
opts.min_angle = 33;

<span class="comment">% domain</span>
tristr.points = [<span class="keyword">...</span>
    -1 1 1 -1;<span class="keyword">...</span>
    -1 -1 1 1];
tristr.segments = uint32([<span class="keyword">...</span>
    1 2 3 4;<span class="keyword">...</span>
    2 3 4 1]);

<span class="comment">% Generate the mesh using triangle</span>
MESH = mtriangle(opts, tristr);
</pre><h2>Build sparse matrices<a name="4"></a></h2><p>We assemble two sparse matrices - general and symmetric - to test the spmv routine. Matrices are assembled for a random symmetric element matrix.</p><pre class="codeinput"><span class="comment">% initialization</span>
ndof   = 3;              <span class="comment">% assemble matrices for ndof dofs per node</span>
nnodel = size(MESH.ELEMS, 1);
nel    = size(MESH.ELEMS, 2);
nelemdof = nnodel*ndof;

<span class="comment">% generate random symmetric element matrix</span>
Aelem   = rand(nelemdof,nelemdof);
Aelem   = Aelem+Aelem';

<span class="comment">% extract lower triangular part from the elemen matrix</span>
Aelem_s = Aelem(find(tril(Aelem)));
</pre><p>Assemble general sparse matrix using the same element matrix for all elements</p><pre class="codeinput">opts.n_node_dof = ndof;
opts.symmetric  = 0;
t = tic;
Ag = sparse_create(MESH.ELEMS, Aelem(:), opts);
display([<span class="string">'assemble general sparse matrix ('</span> num2str(ndof) <span class="string">' dof per node): '</span> num2str(toc(t))]);
spy(Ag)
</pre><pre class="codeoutput">assemble general sparse matrix (3 dof per node): 1.0784
</pre><img vspace="5" hspace="5" src="ex_spmv_01.png" alt=""> <p>Assemble symmetric sparse matrix using the same element matrix for all elements</p><pre class="codeinput">opts.n_node_dof = ndof;
opts.symmetric  = 1;
t = tic;
As = sparse_create(MESH.ELEMS, Aelem_s(:), opts);
display([<span class="string">'assemble symmetric sparse matrix ('</span> num2str(ndof) <span class="string">' dof per node): '</span> num2str(toc(t))]);

<span class="comment">% compare symmetric and general sparse matrices</span>
<span class="keyword">if</span> nnz(As-tril(Ag))
    error(<span class="string">'symmetric and general matrices created by sparse_create differ'</span>);
<span class="keyword">end</span>
</pre><pre class="codeoutput">assemble symmetric sparse matrix (3 dof per node): 0.66414
</pre><h2>Compute communication reducing reordering and permute the matrices<a name="7"></a></h2><p>Parallel spmv requires that the matrix is partitioned among the processors. Depending on the way it is partitioned, the cpus exchange different amount of data during computations. For small matrices and moderate number of CPUs the Reverse Cuthill-McKee reordering is usually a good strategy. For larger number of CPUs and larger systems METIS graph partitioning should be used.</p><pre class="codeinput"><span class="comment">% set number of threads</span>
opts.nthreads = 2;

<span class="comment">% compute the reordering for a 1 dof per node matrix</span>
Aconn = sparse_create(MESH.ELEMS);

<span class="comment">% RCM reordering</span>
t=tic;
perm = mrcm(Aconn);
display([<span class="string">'MRCM:                      '</span> num2str(toc(t))]);

<span class="comment">% METIS graph partitioning</span>
rowdist = [];
<span class="keyword">if</span> opts.nthreads&gt;1

    <span class="comment">% permute with rcm first</span>
    Aconn = Aconn(perm,perm);

    t=tic;
    [perm2,~,rowdist] = metis_part(Aconn, opts.nthreads);
    display([<span class="string">'metis_part                 '</span> num2str(toc(t))]);

    <span class="comment">% fix the row distribution for multiple dofs per node</span>
    rowdist = uint32((rowdist-1)*ndof+1);

    <span class="comment">% merge the permutations</span>
    perm = perm(perm2);
<span class="keyword">end</span>
clear <span class="string">Aconn</span>;
</pre><pre class="codeoutput">MRCM:                      0.015073
metis_part                 0.27212
</pre><p>The above permutation was created for a symbolic matrix with 1 dof per node. In case there are more dofs per node, it needs to be 'expanded'</p><pre class="codeinput"><span class="comment">% block permutation for ndof dofs per node</span>
perm = bsxfun(@plus, -fliplr([0:ndof-1])', ndof*double(perm));
perm = perm(:);
</pre><p>Now the symmetric and general sparse matrices can be permuted using the with the final permutation, which is a combination of RCM and METIS graph partitioning.</p><pre class="codeinput">t=tic;
Ag = cs_permute(Ag,perm,perm);
display([<span class="string">'cs_permute                 '</span> num2str(toc(t))]);
spy(Ag)

t=tic;
As = cs_symperm(As',perm)';
display([<span class="string">'cs_symperm                 '</span> num2str(toc(t))]);
</pre><pre class="codeoutput">cs_permute                 0.81817
cs_symperm                 0.81901
</pre><img vspace="5" hspace="5" src="ex_spmv_02.png" alt=""> <h2>Parallel sparse matrix - vector multiplication<a name="10"></a></h2><p>spmv MEX function works in two modes: * multiply symmetric or general matrix converted by sparse_convert * multiply a native MATLAB general matrix sparse_convert prepares a native MATLAB sparse matrix for parallel Sparse Matrix Vector Multiplication (SpMV). There are several performance advantages of using this function over simply running SPMV for native MATLAB sparse matrices:</p><pre>- uint32 type for row/column indices
- symmetric SpMV
- interleaved and blocked storage
- thread affinity and local memory allocation on NUMA systems</pre><p>Hence, the first approach is significantly faster if many spmv calls have to be performed and the cost of sparse_convert can be amortized.</p><pre class="codeinput"><span class="comment">% convert a symmetric sparse matrix</span>
opts.symmetric  = 1;    <span class="comment">% symmetric storage</span>
opts.block_size = ndof; <span class="comment">% block size to use for Blocked CRS storage</span>
t=tic;
As_converted = sparse_convert(As, opts);
display([<span class="string">'sparse_convert (symmetric, parallel) '</span> num2str(toc(t))]);

<span class="comment">% convert a general sparse matrix</span>
opts.symmetric  = 0;
opts.block_size = ndof;
t=tic;
Ag_converted = sparse_convert(Ag, opts);
display([<span class="string">'sparse_convert (general, parallel)   '</span> num2str(toc(t))]);
</pre><pre class="codeoutput">sparse_convert (symmetric, parallel) 0.050391
sparse_convert (general, parallel)   0.066067
</pre><p>Run the different versions of spmv and compare the times and accuracy of the results</p><pre class="codeinput"><span class="comment">% randomize x vector</span>
x = rand(size(Ag,1), 1);

t = tic;
<span class="keyword">for</span> i=1:100
    v1 = spmv(As_converted, x);
<span class="keyword">end</span>
display([<span class="string">'spmv converted, symmetric (parallel)  '</span> num2str(toc(t))]);

t = tic;
<span class="keyword">for</span> i=1:100
    v2 = spmv(Ag_converted, x);
<span class="keyword">end</span>
display([<span class="string">'spmv converted, general (parallel)    '</span> num2str(toc(t))]);

t = tic;
setenv(<span class="string">'OMP_NUM_THREADS'</span>, <span class="string">'1'</span>);
<span class="keyword">for</span> i=1:100
    v3 = spmv(Ag, x);
<span class="keyword">end</span>
display([<span class="string">'spmv native (sequential)   '</span> num2str(toc(t))]);

t = tic;
setenv(<span class="string">'OMP_NUM_THREADS'</span>, num2str(opts.nthreads));
<span class="keyword">for</span> i=1:100
    v3 = spmv(Ag, x);
<span class="keyword">end</span>
display([<span class="string">'spmv native (parallel)     '</span> num2str(toc(t))]);

t = tic;
<span class="keyword">for</span> i=1:100
    v4 = Ag*x;
<span class="keyword">end</span>
display([<span class="string">'Ag*x                       '</span> num2str(toc(t))]);
setenv(<span class="string">'OMP_NUM_THREADS'</span>, <span class="string">''</span>);
</pre><pre class="codeoutput">spmv converted, symmetric (parallel)  1.1011
spmv converted, general (parallel)    1.5713
spmv native (sequential)   3.6909
spmv native (parallel)     2.8833
Ag*x                       3.93
</pre><pre class="codeinput"><span class="comment">% compare the different results</span>
<span class="keyword">if</span>(norm(v1-v4)&gt;1e-11)
    error(<span class="string">'spmv(As_converted,x) and Ag*x returned different results'</span>);
<span class="keyword">end</span>
<span class="keyword">if</span>(norm(v2-v4)&gt;1e-11)
    error(<span class="string">'spmv(Ag_converted,x) and Ag*x returned different results'</span>);
<span class="keyword">end</span>
<span class="keyword">if</span>(norm(v3-v4)&gt;1e-11)
    error(<span class="string">'spmv(Ag,x) and Ag*x returned different results'</span>);
<span class="keyword">end</span>
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.13<br></p></div><!--
##### SOURCE BEGIN #####
%% Parallel sparse matrix - vector multiplication
% ex_spmv shows how to
%
% * reorder the sparse matrix to be suited for parallel spmv
% * distribute the matrix among threads using sparse_convert
% * run parallel spmv on native MATLAB matrices
%
%%

function ex_spmv

%% Generate unstructured triangular mesh

% triangle options
opts = [];
opts.max_tri_area = 0.000084;
opts.element_type = 'tri7';
opts.gen_edges = 0;
opts.min_angle = 33;

% domain
tristr.points = [...
    -1 1 1 -1;...
    -1 -1 1 1];
tristr.segments = uint32([...
    1 2 3 4;...
    2 3 4 1]);

% Generate the mesh using triangle
MESH = mtriangle(opts, tristr);


%% Build sparse matrices
% We assemble two sparse matrices - general and symmetric - to test the
% spmv routine. Matrices are assembled for a random symmetric element
% matrix. 

% initialization
ndof   = 3;              % assemble matrices for ndof dofs per node
nnodel = size(MESH.ELEMS, 1);
nel    = size(MESH.ELEMS, 2);
nelemdof = nnodel*ndof;

% generate random symmetric element matrix
Aelem   = rand(nelemdof,nelemdof);
Aelem   = Aelem+Aelem';

% extract lower triangular part from the elemen matrix
Aelem_s = Aelem(find(tril(Aelem)));

%%
% Assemble general sparse matrix using the same element matrix for all elements
opts.n_node_dof = ndof;
opts.symmetric  = 0;
t = tic;
Ag = sparse_create(MESH.ELEMS, Aelem(:), opts);
display(['assemble general sparse matrix (' num2str(ndof) ' dof per node): ' num2str(toc(t))]);
spy(Ag)

%% 
% Assemble symmetric sparse matrix using the same element matrix for all elements
opts.n_node_dof = ndof;
opts.symmetric  = 1;
t = tic;
As = sparse_create(MESH.ELEMS, Aelem_s(:), opts);
display(['assemble symmetric sparse matrix (' num2str(ndof) ' dof per node): ' num2str(toc(t))]);

% compare symmetric and general sparse matrices
if nnz(As-tril(Ag))
    error('symmetric and general matrices created by sparse_create differ');
end


%% Compute communication reducing reordering and permute the matrices
% Parallel spmv requires that the matrix is partitioned among the
% processors. Depending on the way it is partitioned, the cpus 
% exchange different amount of data during computations. For small matrices and
% moderate number of CPUs the Reverse Cuthill-McKee reordering is usually a
% good strategy. For larger number of CPUs and larger systems METIS graph
% partitioning should be used.

% set number of threads
opts.nthreads = 2;

% compute the reordering for a 1 dof per node matrix
Aconn = sparse_create(MESH.ELEMS);

% RCM reordering
t=tic;
perm = mrcm(Aconn);
display(['MRCM:                      ' num2str(toc(t))]);

% METIS graph partitioning
rowdist = [];
if opts.nthreads>1

    % permute with rcm first
    Aconn = Aconn(perm,perm);
    
    t=tic;
    [perm2,~,rowdist] = metis_part(Aconn, opts.nthreads);
    display(['metis_part                 ' num2str(toc(t))]);
    
    % fix the row distribution for multiple dofs per node
    rowdist = uint32((rowdist-1)*ndof+1);

    % merge the permutations
    perm = perm(perm2);
end
clear Aconn;

%%
% The above permutation was created for a symbolic matrix with 1 dof per
% node. In case there are more dofs per node, it needs to be 'expanded'

% block permutation for ndof dofs per node
perm = bsxfun(@plus, -fliplr([0:ndof-1])', ndof*double(perm));
perm = perm(:);

%%
% Now the symmetric and general sparse matrices can be permuted
% using the with the final permutation, which is a combination of RCM and
% METIS graph partitioning.
t=tic;
Ag = cs_permute(Ag,perm,perm);
display(['cs_permute                 ' num2str(toc(t))]);
spy(Ag)

t=tic;
As = cs_symperm(As',perm)';
display(['cs_symperm                 ' num2str(toc(t))]);


%% Parallel sparse matrix - vector multiplication
% spmv MEX function works in two modes:
% * multiply symmetric or general matrix converted by sparse_convert
% * multiply a native MATLAB general matrix
% sparse_convert prepares a native MATLAB sparse matrix for parallel 
% Sparse Matrix Vector Multiplication (SpMV). There are several performance 
% advantages of using this function over simply running SPMV for native 
% MATLAB sparse matrices:
%
%  - uint32 type for row/column indices
%  - symmetric SpMV
%  - interleaved and blocked storage
%  - thread affinity and local memory allocation on NUMA systems
%
% Hence, the first approach is significantly faster if many spmv calls have 
% to be performed and the cost of sparse_convert can be amortized.

% convert a symmetric sparse matrix 
opts.symmetric  = 1;    % symmetric storage
opts.block_size = ndof; % block size to use for Blocked CRS storage
t=tic;
As_converted = sparse_convert(As, opts);
display(['sparse_convert (symmetric, parallel) ' num2str(toc(t))]);

% convert a general sparse matrix
opts.symmetric  = 0;
opts.block_size = ndof;
t=tic;
Ag_converted = sparse_convert(Ag, opts);
display(['sparse_convert (general, parallel)   ' num2str(toc(t))]);


%%
% Run the different versions of spmv and compare the times and
% accuracy of the results

% randomize x vector
x = rand(size(Ag,1), 1);

t = tic;
for i=1:100
    v1 = spmv(As_converted, x);
end
display(['spmv converted, symmetric (parallel)  ' num2str(toc(t))]);

t = tic;
for i=1:100
    v2 = spmv(Ag_converted, x);
end
display(['spmv converted, general (parallel)    ' num2str(toc(t))]);

t = tic;
setenv('OMP_NUM_THREADS', '1');
for i=1:100
    v3 = spmv(Ag, x);
end
display(['spmv native (sequential)   ' num2str(toc(t))]);

t = tic;
setenv('OMP_NUM_THREADS', num2str(opts.nthreads));
for i=1:100
    v3 = spmv(Ag, x);
end
display(['spmv native (parallel)     ' num2str(toc(t))]);

t = tic;
for i=1:100
    v4 = Ag*x;
end
display(['Ag*x                       ' num2str(toc(t))]);
setenv('OMP_NUM_THREADS', '');

%%

% compare the different results
if(norm(v1-v4)>1e-11)
    error('spmv(As_converted,x) and Ag*x returned different results');
end
if(norm(v2-v4)>1e-11)
    error('spmv(Ag_converted,x) and Ag*x returned different results');
end
if(norm(v3-v4)>1e-11)
    error('spmv(Ag,x) and Ag*x returned different results');
end

##### SOURCE END #####
--></body></html>